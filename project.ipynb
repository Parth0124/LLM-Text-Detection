{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "287f8c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/parthabhang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2c54d88-204e-4fb7-a340-03079004811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paragraph with masked words:\n",
      "my name is parth . i am a boy . i love to [MASK] football and [MASK] 1 . my favourite team is [MASK] madrid in football and mercedes in [MASK] 1 . i play at a [MASK] [MASK] position in football . i love coding . i have built many projects . some of them are house marketplace and github finder [MASK] with [MASK] sentiment analysis and [MASK] prediction . i am learning [MASK] ai . i am searching for internships to work in during the summer holidays .\n",
      "\n",
      "Is LLM generated: False\n",
      "Accuracy: 0.00\n",
      "\n",
      "Predictions saved to files:\n",
      "- Detailed JSON report: text_analysis_results_[timestamp].json\n",
      "- Predicted words CSV: predicted_words_[timestamp].csv\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def initialize_model():\n",
    "    \"\"\"Initialize BERT model and tokenizer with proper configuration\"\"\"\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\n",
    "        model_name,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "def preprocess_text(paragraph):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = re.findall(r'\\w+|[^\\w\\s]', paragraph.lower()) \n",
    "    return words, stop_words\n",
    "\n",
    "def analyze_text_authenticity(paragraph, num_red_tokens=10):\n",
    "    tokenizer, model = initialize_model()\n",
    "\n",
    "    words, stop_words = preprocess_text(paragraph)\n",
    "\n",
    "    if num_red_tokens > len(words):\n",
    "        num_red_tokens = len(words)\n",
    "\n",
    "    word_indices = [i for i, word in enumerate(words) if re.match(r'\\w+', word) and word not in stop_words]\n",
    "    mask_indices = np.random.choice(word_indices, num_red_tokens, replace=False)\n",
    "\n",
    "    marked_words = words.copy()\n",
    "    for idx in mask_indices:\n",
    "        marked_words[idx] = '[MASK]'\n",
    "\n",
    "    marked_text = ' '.join(marked_words)\n",
    "\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        marked_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        mask_positions = inputs['input_ids'][0] == tokenizer.mask_token_id\n",
    "\n",
    "        predictions = []\n",
    "        for pos in torch.where(mask_positions)[0]:\n",
    "            probs = torch.softmax(logits[0, pos], dim=-1)\n",
    "            top_probs, top_tokens = torch.topk(probs, k=5)\n",
    "            original_word = words[mask_indices[len(predictions)]]\n",
    "            predicted_word = tokenizer.convert_ids_to_tokens(top_tokens[0].item())\n",
    "\n",
    "            top_5_predictions = [\n",
    "                {'token': tokenizer.convert_ids_to_tokens(top_tokens[i].item()), 'probability': top_probs[i].item()}\n",
    "                for i in range(5)\n",
    "            ]\n",
    "\n",
    "            predictions.append({\n",
    "                'original': original_word,\n",
    "                'predicted': predicted_word,\n",
    "                'probability': top_probs[0].item(),\n",
    "                'is_match': original_word == predicted_word,\n",
    "                'top_5_predictions': top_5_predictions\n",
    "            })\n",
    "\n",
    "    correct_predictions = sum(1 for p in predictions if p['is_match'])\n",
    "    total_predictions = len(predictions)\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "    is_llm_generated = accuracy > 0.5\n",
    "\n",
    "    results = {\n",
    "        'predictions': predictions,\n",
    "        'accuracy': accuracy,\n",
    "        'is_llm_generated': is_llm_generated,\n",
    "        'marked_text': marked_text,\n",
    "        'original_text': ' '.join(words)\n",
    "    }\n",
    "\n",
    "    save_results(results)\n",
    "    return results\n",
    "\n",
    "def save_results(results):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    json_filename = f'text_analysis_results_{timestamp}.json'\n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    csv_filename = f'predicted_words_{timestamp}.csv'\n",
    "    with open(csv_filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Original Word', 'Predicted Word', 'Probability', 'Is Match', \n",
    "                         'Top 5 Predictions'])\n",
    "\n",
    "        for pred in results['predictions']:\n",
    "            top_5_str = ' | '.join([f\"{p['token']}({p['probability']:.3f})\" \n",
    "                                     for p in pred['top_5_predictions']])\n",
    "            writer.writerow([\n",
    "                pred['original'],\n",
    "                pred['predicted'],\n",
    "                f\"{pred['probability']:.3f}\",\n",
    "                pred['is_match'],\n",
    "                top_5_str\n",
    "            ])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_paragraph = \"\"\"My name is Parth. I am a boy. I love to watch football and formula 1. My favourite team is Real Madrid in football and Mercedes in Formula 1. I play at a center forward position in football. I love coding. I have built many projects. Some of them are House marketplace and Github finder along with IMDB sentiment analysis and Churn prediction. I am learning generative AI. I am searching for internships to work in during the summer holidays.\"\"\"\n",
    "\n",
    "    results = analyze_text_authenticity(sample_paragraph)\n",
    "    print(f\"\\nParagraph with masked words:\\n{results['marked_text']}\")\n",
    "    print(f\"\\nIs LLM generated: {results['is_llm_generated']}\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.2f}\")\n",
    "    print(\"\\nPredictions saved to files:\")\n",
    "    print(\"- Detailed JSON report: text_analysis_results_[timestamp].json\")\n",
    "    print(\"- Predicted words CSV: predicted_words_[timestamp].csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
